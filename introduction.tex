\noindent

Many data analytics applications, like intrusion detection and web search, require the computation of results in a timely fashion in order to provide interactivity and fast response to incoming events.
Data analytics systems like Storm~\cite{Storm} or Spark Streaming~\cite{SparkStreaming} provide easy-to-use and deploy frameworks for these types of applications. Storm handles streams of data by creating pipelines for record-at-a-time processing with at-least-once semantics. Spark Streaming offers processing capabilities with micro-batches, where records are coalesced into small groups before being consumed together.

% TODO:
% Why are we focusing on Spark Streaming?
%
%
Spark Streaming, the focus of our work, is a stream processing engine built on top of Spark~\cite{Spark}, a fast and general-purpose engine for large-scale data processing. It implements the discretized stream (D-Stream) abstraction, and uses a micro-batch approach to process data as they are received. The application specifies the number of receivers and a batch interval (NOTE: explain what is a batch interval), and Spark Streaming will process all of the data received in the interval by all receivers in a continuous, batched fashion. Every batch job is divided into map and reduce stages, each of which contains tasks that need to be scheduled. Internally, a variable called block interval controls the period of time before a receiver generates a new block. The total number of blocks generated in a batch interval corresponds to the number of tasks spawned for every stage of the resulting batch job.

We perform an in-depth analysis of Spark Streaming, and introduce a number of improvements to the system so that it can handle applications with latency requirements on the order of hundreds or even tens of milliseconds while sacrificing as little as possible on throughput. More specifically, we focus on three main areas of improvements. The first area is to reduce the overhead of running tasks. As tasks are executed, there are a number of inefficiencies in terms of sending duplicate data and performing redundant computations. The second area is the scheduling scalability. Currently, Spark uses a centralized scheduler, which becomes a bottleneck as the number of tasks needed to be scheduled increases with the increase in number of receivers or decrease in the block interval. The third area of focus is the network overhead. The amount of network communication is proportional to the number of tasks running concurrently. We propose an enhancement that uses newer technology such as InfiniBand to reduce the transmission delay, or remote direct memory access (RDMA) to relieve the burden of sending and receiving network packets from the driver CPU.

The paper is organized as follows: in Section 2 we motivate the importance of solving the problems of 1) high task overhead and 2) limited scheduling scalability. In Section 3 we discuss the implementation details on solving these problems. In Section 4 we conclude.

