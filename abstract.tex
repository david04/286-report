\noindent Large-scale data processing systems such as Spark and Hadoop MapReduce were developed for clusters built on commodity hardware. In this paper, we argue that current market trends invalidate some of the assumptions made by those systems, and we should therefore revisit the designs and architectures derived from them. For example, newer technologies including InfiniBand offer faster network connectivity, and advances in SSDs continue to drive down the cost of fast non-volatile memory. One new architecture that explores these new technologies is Firebox, a hardware building block for future Warehouse-Scale Computers (WSCs). We use Firebox in our studies, and evaluate the behavior changes of large-scale systems under the new settings. In particular, we measure the performance of Spark and applications in the Spark ecosystem under specific workloads, and critique the validity of its underlying assumptions and design decisions.
